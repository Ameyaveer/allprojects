The project employs an encoder-decoder architecture, where a pre-trained Convolutional Neural Network (CNN) acts as the encoder to extract high-level features from the input image. The project uses 2 models, a VGG16 with a Bidirectional Long Short-Term Memory (Bi-LSTM) network and InceptionV3 with a standard Long Short-Term Memory (LSTM) network. The encoder CNN extracts visual features, while the decoder RNN leverages these features to generate captions describing the content of the images.
